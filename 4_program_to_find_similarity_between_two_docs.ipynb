{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e70e6cd",
   "metadata": {},
   "source": [
    "## LAB 4: Program to Find the Similarity Between Documents\n",
    "\n",
    "In this lab, we will implement a program to find the similarity between documents using two different approaches: Jaccard similarity and Cosine similarity. We'll preprocess the documents by lowercasing, tokenizing, lemmatizing, and removing stopwords. Then, we'll calculate the similarity between documents using the Jaccard similarity formula and the Cosine similarity formula.\n",
    "\n",
    "### Required Libraries:\n",
    "- `nltk`: For natural language processing tasks such as tokenization, lemmatization, and stopwords removal.\n",
    "- `sklearn`: For TF-IDF vectorization and cosine similarity calculation.\n",
    "\n",
    "### Definitions and Formulas:\n",
    "\n",
    "#### Jaccard Similarity:\n",
    "The Jaccard similarity measures the similarity between two sets by comparing the intersection and union of the sets.\n",
    "\n",
    "The Jaccard' s similarity formula : \n",
    "$J(A,B)=\\cfrac{\\mid A\\cap B\\mid}{\\mid A \\cup B\\mid}$\n",
    "\n",
    "#### Cosine Similarity:\n",
    "Cosine similarity measures the cosine of the angle between two vectors, representing the documents, in a high-dimensional space.\n",
    "\n",
    "The Cosine Similarity Formula : \n",
    "$cos(\\theta) = \\cfrac{A \\cdot B}{\\|A\\| \\|B\\|}$\n",
    "\n",
    "### Steps:\n",
    "1. Preprocess documents: lowercase, tokenize, lemmatize, and remove stopwords.\n",
    "2. Calculate Jaccard similarity between documents.\n",
    "3. Calculate cosine similarity using TF-IDF vectorization.\n",
    "4. Find the most similar document for each document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c81a22",
   "metadata": {},
   "source": [
    "# Ranking with similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fffde9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb2793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The dog loves to play fetch in the park.\",\n",
    "    \"The cat sleeps all day long.\",\n",
    "    \"The sun shines brightly in the sky.\",\n",
    "    \"The moon shines dimly in the night.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc3ea1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess documents: lowercase, tokenize, lemmatize, remove stopwords\n",
    "def preprocess_document(document):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = nltk.word_tokenize(document.lower())\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "\n",
    "# Flatten the list of lists to single strings\n",
    "processed_documents = [' '.join(words) for words in map(preprocess_document, documents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e500bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity Ranking:\n",
      "The sun shines brightly in the sky.\n",
      "The moon shines dimly in the night.\n",
      "The dog loves to play fetch in the park.\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "The cat sleeps all day long.\n",
      "\n",
      "Cosine Similarity Ranking:\n",
      "The sun shines brightly in the sky.\n",
      "The moon shines dimly in the night.\n",
      "The dog loves to play fetch in the park.\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "The cat sleeps all day long.\n"
     ]
    }
   ],
   "source": [
    "# Define your query\n",
    "query = 'sun in the sky'\n",
    "\n",
    "# Tokenize the documents\n",
    "tokenized_documents = [doc.split() for doc in documents]\n",
    "\n",
    "# Create a vocabulary\n",
    "vocabulary = sorted(set(sum(tokenized_documents, [])))\n",
    "\n",
    "# Jaccard Similarity\n",
    "jaccard_similarity = []\n",
    "for doc in tokenized_documents:\n",
    "    intersection = len(set(doc).intersection(set(query.split())))\n",
    "    union = len(set(doc).union(set(query.split())))\n",
    "    jaccard_similarity.append(intersection / union)\n",
    "\n",
    "# Rank documents by Jaccard Similarity\n",
    "jaccard_ranking = [doc for _, doc in sorted(zip(jaccard_similarity, documents), reverse=True)]\n",
    "\n",
    "print(\"Jaccard Similarity Ranking:\")\n",
    "for doc in jaccard_ranking:\n",
    "    print(doc)\n",
    "\n",
    "# Cosine Similarity\n",
    "# Create vectors\n",
    "vectors = [[doc.count(word) for word in vocabulary] for doc in tokenized_documents]\n",
    "\n",
    "# Calculate dot product\n",
    "dot_product = lambda a, b : sum([a[i]*b[i] for i in range(len(a))])\n",
    "\n",
    "# Calculate magnitude\n",
    "magnitude = lambda a : sum([a[i]*a[i] for i in range(len(a))]) ** 0.5\n",
    "\n",
    "# Calculate Cosine Similarity\n",
    "cosine_similarity = []\n",
    "epsilon = 1e-7  # small constant\n",
    "query_vector = [query.count(word) for word in vocabulary]\n",
    "for doc_vector in vectors:\n",
    "    cosine_similarity.append(dot_product(doc_vector, query_vector) / (magnitude(doc_vector) * magnitude(query_vector) + epsilon))\n",
    "\n",
    "\n",
    "# Rank documents by Cosine Similarity\n",
    "cosine_ranking = [doc for _, doc in sorted(zip(cosine_similarity, documents), reverse=True)]\n",
    "\n",
    "print(\"\\nCosine Similarity Ranking:\")\n",
    "for doc in cosine_ranking:\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e27d0d",
   "metadata": {},
   "source": [
    "# Checking Similarity Between docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b30d5e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d9de508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dummy documents\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The dog loves to play fetch in the park.\",\n",
    "    \"The cat sleeps all day long.\",\n",
    "    \"The sun shines brightly in the sky.\",\n",
    "    \"The moon shines dimly in the night.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02d1f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess documents: lowercase, tokenize, lemmatize, remove stopwords\n",
    "def preprocess_document(document):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = nltk.word_tokenize(document.lower())\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "\n",
    "# Flatten the list of lists to single strings\n",
    "processed_documents = [' '.join(words) for words in map(preprocess_document, documents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65d967fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard similarity\n",
    "def jaccard_similarity(doc1, doc2):\n",
    "    intersection = len(set(doc1).intersection(set(doc2)))\n",
    "    union = len(set(doc1).union(set(doc2)))\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3336ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard similarity matrix:\n",
      "1.0000 0.4815 0.4231 0.4074 0.3704 \n",
      "0.4815 1.0000 0.6316 0.4545 0.4762 \n",
      "0.4231 0.6316 1.0000 0.4500 0.6471 \n",
      "0.4074 0.4545 0.4500 1.0000 0.6111 \n",
      "0.3704 0.4762 0.6471 0.6111 1.0000 \n"
     ]
    }
   ],
   "source": [
    "# Print similarity matrices\n",
    "print(\"Jaccard similarity matrix:\")\n",
    "for i in range(len(documents)):\n",
    "    for j in range(len(documents)):\n",
    "        print(f\"{jaccard_similarity(processed_documents[i], processed_documents[j]):.4f}\", end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36d3f4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_documents)\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffa51394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine similarity matrix:\n",
      "[[1.         0.1269686  0.         0.         0.        ]\n",
      " [0.1269686  1.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.17828843]\n",
      " [0.         0.         0.         0.17828843 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCosine similarity matrix:\")\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef109345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1 is most similar to document 2 with similarity: 0.1270\n",
      "Similar document: The dog loves to play fetch in the park.\n",
      "\n",
      "Document 2 is most similar to document 1 with similarity: 0.1270\n",
      "Similar document: The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "Document 3 is most similar to document 5 with similarity: 0.0000\n",
      "Similar document: The moon shines dimly in the night.\n",
      "\n",
      "Document 4 is most similar to document 5 with similarity: 0.1783\n",
      "Similar document: The moon shines dimly in the night.\n",
      "\n",
      "Document 5 is most similar to document 4 with similarity: 0.1783\n",
      "Similar document: The sun shines brightly in the sky.\n"
     ]
    }
   ],
   "source": [
    "# Find most similar document for each document\n",
    "for i in range(len(documents)):\n",
    "    most_similar_index = similarity_matrix[i].argsort()[-2]\n",
    "    most_similar_doc = documents[most_similar_index]\n",
    "    similarity = similarity_matrix[i, most_similar_index]\n",
    "    print(f\"\\nDocument {i+1} is most similar to document {most_similar_index+1} with similarity: {similarity:.4f}\")\n",
    "    print(f\"Similar document: {most_similar_doc}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
